{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQL4Y5oSsLP77speReOcEU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTJ4j3PcKLy-","executionInfo":{"status":"ok","timestamp":1734954054732,"user_tz":-120,"elapsed":1316660,"user":{"displayName":"Ярослав Білотіл","userId":"06797585363548962278"}},"outputId":"87929676-a5e5-4b14-d8ee-5d9d7fc5b974"},"outputs":[{"output_type":"stream","name":"stdout","text":["Епоха 100: Loss=1945.89185298, k=1.9244, b=1.0180\n","Епоха 200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 1900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 2900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 3900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 4900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 5900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 6900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 7900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 8900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 9900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 10900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 11900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 12900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 13900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 14900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 15900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 16900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 17900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 18900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19000: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19100: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19200: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19300: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19400: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19500: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19600: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19700: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19800: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 19900: Loss=1945.89185461, k=1.9244, b=1.0180\n","Епоха 20000: Loss=1945.89185461, k=1.9244, b=1.0180\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","\n","# 1. Генерація випадкових даних X та y\n","n_samples, batch_size, num_steps = 1000, 100, 20000\n","X_data = np.random.uniform(0, 1, (n_samples, 1))  # 1000 випадкових точок на [0; 1]\n","y_data = 2 * X_data + 1 + np.random.normal(0, np.sqrt(2), (n_samples, 1))  # y = 2x + 1 + шум\n","\n","# 2. Створюємо змінні для X та y, використовуючи tf.data.Dataset для обробки батчів\n","dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data)).batch(batch_size)\n","\n","# 3. Ініціалізація параметрів k та b з малими значеннями\n","k = tf.Variable(tf.random.normal([1, 1], stddev=0.1, dtype=tf.float64), name='slope')  # Сlope (коефіцієнт)\n","b = tf.Variable(tf.zeros([1], dtype=tf.float64), name='bias')  # Зсув (константа)\n","\n","# 4. Прогнозування (y_pred = k * X + b)\n","def model(X):\n","    # Перевести X в тип float64 перед матричним множенням\n","    X = tf.cast(X, dtype=tf.float64)\n","    return tf.matmul(X, k) + b  # Виконати операцію без додаткового dtype\n","\n","# 5. Функція втрат (сума квадратів похибок)\n","def loss_fn(y_true, y_pred):\n","    return tf.reduce_sum((y_true - y_pred) ** 2)\n","\n","# 6. Оптимізатор (стохастичний градієнтний спуск)\n","optimizer = tf.optimizers.SGD(learning_rate=0.001)  # Зменшена швидкість навчання\n","\n","# 7. Тренувальний цикл\n","for epoch in range(num_steps):\n","    epoch_loss = 0\n","    for X_batch, y_batch in dataset:\n","        # Перевести X_batch та y_batch в тип float64\n","        X_batch = tf.cast(X_batch, dtype=tf.float64)\n","        y_batch = tf.cast(y_batch, dtype=tf.float64)\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = model(X_batch)  # Передбачення\n","            loss_value = loss_fn(y_batch, y_pred)  # Обчислення втрат\n","\n","        gradients = tape.gradient(loss_value, [k, b])  # Обчислення градієнтів\n","\n","        # Оновлення параметрів\n","        optimizer.apply_gradients(zip(gradients, [k, b]))\n","\n","        epoch_loss += loss_value\n","\n","    # Виведення результатів кожні 100 епох\n","    if (epoch + 1) % 100 == 0:\n","        print(f'Епоха {epoch + 1}: Loss={epoch_loss.numpy():.8f}, k={k.numpy()[0][0]:.4f}, b={b.numpy()[0]:.4f}')\n","\n","    # Перевірка на NaN та Inf\n","    if np.isnan(epoch_loss.numpy()) or np.isinf(epoch_loss.numpy()):\n","        print(f\"Warning: Loss is NaN or Inf at epoch {epoch+1}.\")\n","        break\n"]}]}